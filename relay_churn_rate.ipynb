{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relay Churn Rate\n",
    "\n",
    "The goal with this project is to build up some intuition about the current churn rates within The Tor Network.\n",
    "\n",
    "The Tor Network is an open network, which means that anybody with access to a networked computer with a semi-static IP address can set it up to participate as a relay in the network.\n",
    "\n",
    "We currently don't have a good overview of how long relays stay in the Tor network and how many new nodes joins the network each month.\n",
    "\n",
    "Before we can dive into this, we need to be clear about some definition. When we talk about \"Relay Churn Rate\" here, we are interested in seeing how many nodes that leaves the network. We look at the network on a monthly basis, so if a relay have participated in the network for a single day, the relay will be considered to have been active during that month.\n",
    "\n",
    "We are interested in figuring out:\n",
    "\n",
    "1. How many never before seen (new) relays are joining the Tor network each month?\n",
    "2. How many relays have left the network in the given month (but was active last month)?\n",
    "3. How many relays are returning from having been away from the network for an entire month?\n",
    "\n",
    "Additionally we are interested in learning about the following:\n",
    "\n",
    "1. How many unique relays have the Tor network seen in total during its lifetime?\n",
    "2. What is the lifetime properties of a relay? For example: The average lifetime of a relay?\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The first thing we have to do is to build up the data structure from the historical data in the archives. We use the Stem library to parse the historical data into a Python data structure that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stem.descriptor import parse_file\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import os\n",
    "import binascii\n",
    "import logging\n",
    "\n",
    "DATA_DIR = \"/home/user/stem-collector-data\"\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build our data structure that we are going to use for the computations later. The resulting structure will be a map of a `(year, month)` tuple to a set of relay fingerprints.\n",
    "\n",
    "This part is going to read around 120 GB of data, so it takes around 6 hours on my laptop. The pickled result is around 63 MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-10 21:40:11,750 Parsing: consensuses-2007-10.tar\n",
      "2020-05-10 21:40:26,927 Parsing: consensuses-2007-11.tar\n",
      "2020-05-10 21:42:04,988 Parsing: consensuses-2007-12.tar\n",
      "2020-05-10 21:44:13,879 Parsing: consensuses-2008-01.tar\n"
     ]
    }
   ],
   "source": [
    "def parse_one_consensus(path):\n",
    "    result = {}\n",
    "    year, month = tuple(path.strip(\"consensus-\").strip(\".tar\").split(\"-\"))\n",
    "    \n",
    "    year = int(year)\n",
    "    month = int(month)\n",
    "    \n",
    "    logging.info(\"Parsing: {}\".format(path))\n",
    "    \n",
    "    for relay in parse_file(\"{}/{}\".format(DATA_DIR, path)):\n",
    "        fingerprint = binascii.unhexlify(relay.fingerprint)\n",
    "        \n",
    "        if \"Running\" not in relay.flags:\n",
    "            continue\n",
    "            \n",
    "        result[fingerprint] = {\n",
    "            \"nickname\": relay.nickname,\n",
    "        }\n",
    "    \n",
    "    return (year, month, result) \n",
    "\n",
    "files = sorted(os.listdir(DATA_DIR))\n",
    "result = {}\n",
    "metadata = {}\n",
    "\n",
    "for year, month, relays in map(parse_one_consensus, files[0:4]):\n",
    "    result[year, month] = set(relays.keys())\n",
    "    metadata[year, month] = relays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the computed data on disk since parsing the files with stem takes forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(result, open(\"result.pickle\", \"wb\"))\n",
    "pickle.dump(metadata, open(\"metadata.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the data we have collected into a Panda DataFrame using various set operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>relay_count_sum</th>\n",
       "      <th>relay_count</th>\n",
       "      <th>relay_new_count</th>\n",
       "      <th>relay_loss_count</th>\n",
       "      <th>relay_return_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>3239</td>\n",
       "      <td>3239</td>\n",
       "      <td>3239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-11-01</td>\n",
       "      <td>9273</td>\n",
       "      <td>8487</td>\n",
       "      <td>6034</td>\n",
       "      <td>786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-12-01</td>\n",
       "      <td>14650</td>\n",
       "      <td>8866</td>\n",
       "      <td>5377</td>\n",
       "      <td>5035</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>20364</td>\n",
       "      <td>9462</td>\n",
       "      <td>5714</td>\n",
       "      <td>5313</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  relay_count_sum  relay_count  relay_new_count  relay_loss_count  \\\n",
       "0 2007-10-01             3239         3239             3239                 0   \n",
       "1 2007-11-01             9273         8487             6034               786   \n",
       "2 2007-12-01            14650         8866             5377              5035   \n",
       "3 2008-01-01            20364         9462             5714              5313   \n",
       "\n",
       "   relay_return_count  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                  37  \n",
       "3                 195  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "all_seen_relays = set()\n",
    "\n",
    "last_month = set()\n",
    "\n",
    "output = {\n",
    "    \"date\": [],\n",
    "    \"relay_count_sum\": [],\n",
    "    \"relay_count\": [],\n",
    "    \"relay_new_count\": [],\n",
    "    \"relay_loss_count\": [],\n",
    "    \"relay_return_count\": [],\n",
    "}\n",
    "\n",
    "for key in sorted(result.keys()):\n",
    "    current_month = result[key]\n",
    "    year, month = key\n",
    "    \n",
    "    # Stats.\n",
    "    relay_count = len(current_month)\n",
    "    relay_new_count = len(current_month - all_seen_relays)\n",
    "    relay_loss_count = len(last_month - current_month)\n",
    "    \n",
    "    relay_return_count = len((current_month - last_month) & all_seen_relays)\n",
    "    \n",
    "    # The sum of relays.\n",
    "    all_seen_relays.update(current_month)\n",
    "    relay_count_sum = len(all_seen_relays)\n",
    "        \n",
    "    output[\"date\"].append(datetime.datetime(year, month, 1))\n",
    "    output[\"relay_count\"].append(relay_count)\n",
    "    output[\"relay_count_sum\"].append(relay_count_sum)\n",
    "    output[\"relay_new_count\"].append(relay_new_count)\n",
    "    output[\"relay_loss_count\"].append(relay_loss_count)\n",
    "    output[\"relay_return_count\"].append(relay_return_count)\n",
    "\n",
    "    last_month = current_month\n",
    "    \n",
    "pd.DataFrame.from_dict(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns above tries to explain the following values:\n",
    "\n",
    "- *date*: The first day of the given month.\n",
    "- *relay_count_sum*: The total sum of different, unique, relays we have seen in the consensus at this given point in time.\n",
    "- *relay_count*: The number of different, unique, relays we have seen in the consensus in the given month.\n",
    "- *relay_new_count*: The number of relays that we have not seen before.\n",
    "- *relay_return_count*: The number of relays that we have seen before, but that was not found last month, but has now returned.\n",
    "- *relay_loss_count*: The number of relays that we saw last month, but that are gone in the current month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
